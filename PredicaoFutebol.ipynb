{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredicaoFutebol",
      "provenance": [],
      "authorship_tag": "ABX9TyM7sOYh/6Cyn7y/v3NuTRrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kauecapellato/artigo_ic/blob/main/PredicaoFutebol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I409L8hPSkNB"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3exXmO2zKpAy"
      },
      "source": [
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw97412aSUlx"
      },
      "source": [
        "# Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9vMa9pLStkO"
      },
      "source": [
        "def getTeams(URL_season):\r\n",
        "    req = requests.get(URL_season)\r\n",
        "\r\n",
        "    # Check if you have enterded the URL.\r\n",
        "    if req.status_code == 200: \r\n",
        "        # Creates soup.\r\n",
        "        soup = BeautifulSoup(req.text, 'html.parser')\r\n",
        "\r\n",
        "        # Gets the table (table, in html) complete, by the \"key\" that is the name of the class. \r\n",
        "        teams_table = soup.find_all(\"table\", class_=\"standard_tabelle\")\r\n",
        "        \r\n",
        "        # Breaks the table string into the three rows (rows of the table). \r\n",
        "        teams_lines = str(teams_table[0]).split(\"<tr>\")\r\n",
        "        \r\n",
        "        #  Dictionary whose key is the name of the time and the value is the url of it. \r\n",
        "        dict_names_url = {} \r\n",
        "\r\n",
        "        # Scrolls through the rows of the table and searches for the time name and its url, after which it saves in the dictionary above. \r\n",
        "        for column in teams_lines:\r\n",
        "            if \"td align=\\\"center\\\"\" in column:\r\n",
        "                team_name = column.split(\"href=\\\"\")[1].split(\"\\\">\")[1].split(\"alt=\\\"\")[1].split(\"\\\"\")[0]\r\n",
        "                team_url = column.split(\"href=\\\"\")[1].split(\"\\\">\")[0]\r\n",
        "                dict_names_url[team_name] = team_url\r\n",
        "\r\n",
        "        return dict_names_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHJRv0gsS5ZU"
      },
      "source": [
        "def getNotices(dict_names_url):\r\n",
        "    URL_default = \"https://www.worldfootball.net/\"\r\n",
        "    \r\n",
        "    # Gets news for each team in the dictionary.\r\n",
        "    for team_url in dict_names_url:\r\n",
        "        # Creates the dictionary where the information will be.\r\n",
        "        dict_notices = {'date': [], 'hour': [], 'title': [], 'subtitle': [], 'text': []}\r\n",
        "\r\n",
        "        # Enter the first interaction of while.\r\n",
        "        news_table = True \r\n",
        "        \r\n",
        "        # Used to change the page number of the news. \r\n",
        "        cont_page = 1\r\n",
        "        \r\n",
        "        while news_table != []:\r\n",
        "            try:\r\n",
        "                # Gets the url with the informations according to cont_page\r\n",
        "                req = requests.get(URL_default+dict_names_url[team_url].replace(\"/teams/\", \"news/\"+str(cont_page)+\"/\"))\r\n",
        "                \r\n",
        "                # Check if you have enterded the URL.\r\n",
        "                if req.status_code == 200:\r\n",
        "                    # Creates soup.\r\n",
        "                    soup1 = BeautifulSoup(req.text, 'html.parser')\r\n",
        "\r\n",
        "                    # Gets table with the URLs in the news.\r\n",
        "                    news_table = soup1.find_all(\"div\", class_=\"wfb-news-general\")\r\n",
        "                    soup_news_table = BeautifulSoup(str(news_table[0]), 'html.parser')\r\n",
        "                    news_list = soup_news_table.find_all(\"div\", class_=\"module module-newmon module-newmon-default\")\r\n",
        "                    soup_news_list = BeautifulSoup(str(news_list[0]), 'html.parser')\r\n",
        "                    news_list1 = soup_news_list.find_all(\"div\", class_=\"module-newmon-image\")\r\n",
        "\r\n",
        "                    news_urls = []\r\n",
        "                    # Stores news urls in news_urls. \r\n",
        "                    for news in news_list1:\r\n",
        "                        soup_news_list1 = BeautifulSoup(str(news), 'html.parser')\r\n",
        "                        news_url = soup_news_list1.find_all(\"a\")\r\n",
        "                        news_urls.append(str(news_url[0]).split(\"href=\\\"\")[1].split(\"\\\"\")[0])\r\n",
        "\r\n",
        "                    # Each news_url has on average 20 different news. And each news one different URL.\r\n",
        "                    for url in news_urls:\r\n",
        "                        # Gets each news URL\r\n",
        "                        url_news = url\r\n",
        "                        req_news = requests.get(URL_default+url_news)\r\n",
        "\r\n",
        "                        # Check if you have entered the url\r\n",
        "                        if req_news.status_code == 200:\r\n",
        "                            # Create soup.\r\n",
        "                            soup2 = BeautifulSoup(req_news.text, 'html.parser')\r\n",
        "\r\n",
        "                            # Gets the box where it is the date, subtitle and the news.\r\n",
        "                            news_box = soup2.find_all(\"div\", class_=\"box\")                            \r\n",
        "                            soup_box = BeautifulSoup(str(news_box[0]), 'html.parser')\r\n",
        "                            news_date = soup_box.find_all(\"div\", class_=\"wfb-news-date\")\r\n",
        "                            news_date = str(news_date[0]).split(\"\\n\\t\\t\")[1].split(\"h\\n\\t\")[0]\r\n",
        "                            news_date, news_hour, _ = news_date.split(\" \")\r\n",
        "                            \r\n",
        "                            # Gets the title.\r\n",
        "                            news_title = soup_box.find_all(\"h1\", class_=\"wfb-news-title\")\r\n",
        "                            news_title = str(news_title[0]).split(\"\\\">\")[1].split(\"</h1>\")[0]\r\n",
        "                            \r\n",
        "                            # Find the box content, where it has the subtitle and the news. \r\n",
        "                            news_content = soup_box.find_all(\"div\", class_=\"wfb-news-content\")\r\n",
        "                            news_paragraphs = BeautifulSoup(str(news_content[0]), 'html.parser')\r\n",
        "                            news_paragraphs = news_paragraphs.find_all(\"p\")\r\n",
        "\r\n",
        "                            # Gets the subttitle.\r\n",
        "                            news_subtitle = str(news_paragraphs[0]).split(\"<p>\")[1].split(\"</p>\")[0]\r\n",
        "                            news_text = \"\"\r\n",
        "                            \r\n",
        "                            # Gets all paragraphs from the news. Disgard the 0 because is the subtitle.\r\n",
        "                            for paragraph in range(1,len(news_paragraphs)):\r\n",
        "                                news_text += str(news_paragraphs[paragraph]).split(\"<p>\")[1].split(\"</p>\")[0]\r\n",
        "                                news_text += \" \"\r\n",
        "\r\n",
        "                            # Appends date, hour, subtitle, title and text in the dictionary\r\n",
        "                            dict_notices['date'].append(news_date.replace(\".\", \"/\"))\r\n",
        "                            dict_notices['hour'].append(news_hour)\r\n",
        "                            dict_notices['title'].append(news_title)\r\n",
        "                            dict_notices['subtitle'].append(news_subtitle)\r\n",
        "                            dict_notices['text'].append(news_text.strip())\r\n",
        "                    # Next page\r\n",
        "                    cont_page += 1\r\n",
        "            except:\r\n",
        "                # Next page\r\n",
        "                cont_page += 1\r\n",
        "                pass\r\n",
        "\r\n",
        "        # Creates dataframe before takes the next team.\r\n",
        "        df = pd.DataFrame(dict_notices, columns=['date', 'hour', 'title', 'subtitle', 'text'])      \r\n",
        "        # path where to save .         \r\n",
        "        PATH = f'C:\\\\Users\\\\Peterson\\\\Desktop\\\\IC\\\\{team_url}.csv'\r\n",
        "        # Save as csv.\r\n",
        "        df.to_csv(PATH)\r\n",
        "        print(time_url + \": OK\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}